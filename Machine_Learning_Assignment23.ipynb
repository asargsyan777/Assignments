{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What are the key reasons for reducing the dimensionality of a dataset? What are the major\n",
    "disadvantages?\n",
    "\n",
    "PCA tends to find linear correlations between variables, which is sometimes undesirable. PCA fails in cases where mean and covariance are not enough to define datasets. We may not know how many principal components to keep- in practice, some thumb rules are applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3cc01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What is the dimensionality curse?\n",
    "\n",
    "What is the curse of dimensionality problem?\n",
    "Image result for What is the dimensionality curse?\n",
    "The curse of dimensionality basically means that the error increases with the increase in the number of features. It refers to the fact that algorithms are harder to design in high dimensions and often have a running time exponential in the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec980872",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how\n",
    "can you go about doing it? If not, what is the reason?\n",
    "\n",
    "No, dimensionality reduction is not reversible in general. It loses information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c6b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?\n",
    "\n",
    "PCA can be used to significantly reduce the dimensionality of most datasets, even if they are highly nonlinear because it can at least get rid of useless dimensions. However, if there are no useless dimensions, reducing dimensionality with PCA will lose too much information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d53211",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Assume youre running PCA on a 1,000-dimensional dataset with a 95 percent explained variance\n",
    "ratio. What is the number of dimensions that the resulting dataset would have?\n",
    "hard to say, it depends on dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4efd7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?\n",
    "\n",
    "Vanilla PCA: the dataset fit in memory\n",
    "Incremental PCA: larget dataset that don't fit in memory, online taks\n",
    "Randomized PCA: considerably reduce dimensionality and the dataset fit the memory.\n",
    "kenrl PCA: used for nonlinear PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf13e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. How do you assess a dimensionality reduction algorithms success on your dataset?\n",
    "\n",
    "measure the reconstruction error\n",
    "measure the performance in second Machine Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Is it logical to use two different dimensionality reduction algorithms in a chain?\n",
    "Yes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
