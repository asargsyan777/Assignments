{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Is it OK to initialize all the weights to the same value as long as that value is selected\n",
    "randomly using He initialization?\n",
    "\n",
    "each hidden unit will get exactly the same signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80439e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Name three advantages of the SELU activation function over ReLU.\n",
    "\n",
    "Similar to ReLUs, SELUs enable deep neural networks since there is no problem with vanishing gradients.\n",
    "In contrast to ReLUs, SELUs cannot die.\n",
    "SELUs on their own learn faster and better than other activation functions, even if they are combined with batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. In which cases would you want to use each of the following activation functions: SELU, leaky\n",
    "ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f595a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
    "when using an SGD optimizer?\n",
    "\n",
    "If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Name three ways you can produce a sparse model.\n",
    "\n",
    "Some versions of machine learning models are robust towards sparse data and may be used instead of changing the dimensionality of the data. For example, the entropy-weighted k-means algorithm is better suited to this problem than the regular k-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d6c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
    "new instances)? What about MC Dropout?\n",
    "\n",
    "MC Dropout is exactly like dropout during training, but it is still active during inference, so each inference is slowed down slightly. More importantly, when using MC Dropout you generally want to run inference 10 times or more to get better predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1354e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8297cf5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
