{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c50701",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is feature engineering, and how does it work? Explain the various aspects of feature\n",
    "engineering in depth.\n",
    "\n",
    "Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What is feature selection, and how does it work? What is the aim of it? What are the various\n",
    "methods of function selection?\n",
    "\n",
    "Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af57a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Describe the function selection filter and wrapper approaches. State the pros and cons of each\n",
    "approach?\n",
    "\n",
    "There are three types of feature selection: Wrapper methods (forward, backward, and stepwise selection), Filter methods (ANOVA, Pearson correlation, variance thresholding), and Embedded methods (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f71381",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.What are common methods of feature extraction?\n",
    "\n",
    "\n",
    "The most common linear methods for feature extraction are Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA). PCA uses an orthogonal transformation to convert data into a lower-dimensional space while maximizing the variance of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62813145",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74596c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
    "two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in\n",
    "cosine.\n",
    "\n",
    "Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure document similarity in text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
    "calculate the Hamming gap.\n",
    "\n",
    "The Hamming distance d(10101, 11110) is 3 because 10101 âŠ• 11110 is 01011 (three 1s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b8a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. State what is meant by \"high-dimensional data set\"? Could you offer a few real-life examples?\n",
    "What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
    "What can be done about it?\n",
    "\n",
    "High dimensional data refers to a dataset in which the number of features p is larger than the number of observations N, often written as p >> N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Make a few quick notes on:\n",
    "    PCA is an acronym for Personal Computer Analysis.\n",
    "    \n",
    "    PCA is the simplest of the true eigenvector-based multivariate analyses and is closely related to factor analysis. Factor analysis typically incorporates more ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Make a comparison between:\n",
    "    \n",
    "    1. Sequential backward exclusion vs. sequential forward selection--In forward selection you start with your null model and add predictors. In backward selection you start with a full model including all your variables and then you drop those you do not need/ are not significant 1 at a time.\n",
    "    2. Function selection methods: filter vs. wrapper --The main differences between the filter and wrapper methods for feature selection are: Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
